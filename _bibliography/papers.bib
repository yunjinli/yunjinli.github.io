---
---
@article{li2025sadg,
        title={SADG: Segment Any Dynamic Gaussians Without Object Trackers},
        author={Li, Yun-Jin and Gladkova, Mariia and Xia, Yan and Cremers, Daniel},
        abstract={Understanding dynamic 3D scenes is fundamental for various applications, including extended reality (XR) and autonomous driving. Effectively integrating semantic information into 3D reconstruction enables holistic representation that opens opportunities for immersive and interactive applications. To this end, we introduce SADG, Segment Any Dynamic Gaussian Without Object Trackers, a novel approach that combines dynamic Gaussian Splatting representation and semantic information without reliance on object IDs. We propose to learn semantically-aware features by leveraging masks generated from the Segment Anything Model (SAM) and utilizing our novel contrastive learning objective based on hard pixel mining. The learned Gaussian features can be effectively clustered without further post-processing. This enables fast computation for further object-level editing, such as object removal, composition, and style transfer by manipulating the Gaussians in the scene. Due to the lack of consistent evaluation protocol, we extend several dynamic novel-view datasets with segmentation benchmarks that allow testing of learned feature fields from unseen viewpoints. We evaluate SADG on proposed benchmarks and demonstrate the superior performance of our approach in segmenting objects within dynamic scenes along with its effectiveness for further downstream editing tasks.},
        journal={arXiv preprint arXiv:2411.19290},
        year={2024},
        html={https://yunjinli.github.io/project-sadg/},
        url={https://arxiv.org/abs/2411.19290},
        pdf={https://arxiv.org/abs/2411.19290.pdf},
        selected={true},
        preview={sadg.gif}
      }
      
@article{li2024vxp,
        title={VXP: Voxel-Cross-Pixel Large-scale Image-LiDAR Place Recognition},
        author={Li, Yun-Jin and Gladkova, Mariia and Xia, Yan and Wang, Rui and Cremers, Daniel},
        abstract={Recent works on the global place recognition treat the task as a retrieval problem, where an off-the-shelf global descriptor is commonly designed in image-based and LiDAR-based modalities. However, it is non-trivial to perform accurate image-LiDAR global place recognition since extracting consistent and robust global descriptors from different domains (2D images and 3D point clouds) is challenging. To address this issue, we propose a novel Voxel-Cross-Pixel (VXP) approach, which establishes voxel and pixel correspondences in a self-supervised manner and brings them into a shared feature space. Specifically, VXP is trained in a two-stage manner that first explicitly exploits local feature correspondences and enforces similarity of global descriptors. Extensive experiments on the three benchmarks (Oxford RobotCar, ViViD++ and KITTI) demonstrate our method surpasses the state-of-the-art cross-modal retrieval by a large margin. The code will be publicly available.},
        journal={2025 International Conference on 3D Vision (3DV)},
        year={2025},
        html={https://yunjinli.github.io/projects-vxp/},
        url={https://arxiv.org/abs/2403.14594},
        pdf={https://arxiv.org/pdf/2403.14594.pdf},
        selected={true},
        preview={vxp.gif}
      }

@article{xia2024uniloc,
        title={UniLoc: Towards Universal Place Recognition Using Any Single Modality}, 
        author={Yan Xia and Zhendong Li and  Yun-Jin Li and Letian Shi and Hu Cao and Jo√£o F. Henriques and Daniel Cremers},
        abstract={To date, most place recognition methods focus on single-modality retrieval. While they perform well in specific environments, cross-modal methods offer greater flexibility by allowing seamless switching between map and query sources. It also promises to reduce computation requirements by having a unified model, and achieving greater sample efficiency by sharing parameters. In this work, we develop a universal solution to place recognition, UniLoc, that works with any single query modality (natural language, image, or point cloud). UniLoc leverages recent advances in large-scale contrastive learning, and learns by matching hierarchically at two levels: instance-level matching and scene-level matching. Specifically, we propose a novel Self-Attention based Pooling (SAP) module to evaluate the importance of instance descriptors when aggregated into a place-level descriptor. Experiments on the KITTI-360 dataset demonstrate the benefits of cross-modality for place recognition, achieving superior performance in cross-modal settings and competitive results also for uni-modal scenarios. The code will be available upon acceptance.},
        journal={arXiv preprint arXiv:2412.12079},
        year={2024},
        html={https://yan-xia.github.io/projects/UniLoc/},
        url={https://arxiv.org/abs/2412.12079},
        pdf={https://arxiv.org/pdf/2412.12079},
        selected={true},
        preview={uniloc.gif}
      }