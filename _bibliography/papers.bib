---
---
@article{li2025sadg,
        title={TRASE: Tracking-free 4D Segmentation and Editing},
        author={Li, Yun-Jin and Gladkova, Mariia and Xia, Yan and Cremers, Daniel},
        abstract={Understanding dynamic 3D scenes is crucial for extended reality (XR) and autonomous driving. Incorporating semantic information into 3D reconstruction enables holistic scene representations, unlocking immersive and interactive applications. To this end, we introduce TRASE, a novel tracking-free 4D segmentation method for dynamic scene understanding. TRASE learns a 4D segmentation feature field in a weakly-supervised manner, leveraging a soft-mined contrastive learning objective guided by SAM masks. The resulting feature space is semantically coherent and well-separated, and final object-level segmentation is obtained via unsupervised clustering. This enables fast editing, such as object removal, composition, and style transfer, by directly manipulating the scene's Gaussians. We evaluate TRASE on five dynamic benchmarks, demonstrating state-of-the-art segmentation performance from unseen viewpoints and its effectiveness across various interactive editing tasks.},
        journal={2026 International Conference on 3D Vision (3DV)},
        year={2024},
        html={https://yunjinli.github.io/project-sadg/},
        url={https://arxiv.org/abs/2411.19290},
        pdf={https://arxiv.org/abs/2411.19290.pdf},
        selected={true},
        preview={sadg.gif}
      }
      
@article{li2024vxp,
        title={VXP: Voxel-Cross-Pixel Large-scale Image-LiDAR Place Recognition},
        author={Li, Yun-Jin and Gladkova, Mariia and Xia, Yan and Wang, Rui and Cremers, Daniel},
        abstract={Recent works on the global place recognition treat the task as a retrieval problem, where an off-the-shelf global descriptor is commonly designed in image-based and LiDAR-based modalities. However, it is non-trivial to perform accurate image-LiDAR global place recognition since extracting consistent and robust global descriptors from different domains (2D images and 3D point clouds) is challenging. To address this issue, we propose a novel Voxel-Cross-Pixel (VXP) approach, which establishes voxel and pixel correspondences in a self-supervised manner and brings them into a shared feature space. Specifically, VXP is trained in a two-stage manner that first explicitly exploits local feature correspondences and enforces similarity of global descriptors. Extensive experiments on the three benchmarks (Oxford RobotCar, ViViD++ and KITTI) demonstrate our method surpasses the state-of-the-art cross-modal retrieval by a large margin. The code will be publicly available.},
        journal={2025 International Conference on 3D Vision (3DV)},
        year={2025},
        html={https://yunjinli.github.io/projects-vxp/},
        url={https://arxiv.org/abs/2403.14594},
        pdf={https://arxiv.org/pdf/2403.14594.pdf},
        selected={true},
        preview={vxp.gif}
      }

@article{xia2024uniloc,
        title={UniLoc: Towards Universal Place Recognition Using Any Single Modality}, 
        author={Yan Xia and Zhendong Li and  Yun-Jin Li and Letian Shi and Hu Cao and Jo√£o F. Henriques and Daniel Cremers},
        abstract={To date, most place recognition methods focus on single-modality retrieval. While they perform well in specific environments, cross-modal methods offer greater flexibility by allowing seamless switching between map and query sources. It also promises to reduce computation requirements by having a unified model, and achieving greater sample efficiency by sharing parameters. In this work, we develop a universal solution to place recognition, UniLoc, that works with any single query modality (natural language, image, or point cloud). UniLoc leverages recent advances in large-scale contrastive learning, and learns by matching hierarchically at two levels: instance-level matching and scene-level matching. Specifically, we propose a novel Self-Attention based Pooling (SAP) module to evaluate the importance of instance descriptors when aggregated into a place-level descriptor. Experiments on the KITTI-360 dataset demonstrate the benefits of cross-modality for place recognition, achieving superior performance in cross-modal settings and competitive results also for uni-modal scenarios. The code will be available upon acceptance.},
        journal={arXiv preprint arXiv:2412.12079},
        year={2024},
        html={https://yan-xia.github.io/projects/UniLoc/},
        url={https://arxiv.org/abs/2412.12079},
        pdf={https://arxiv.org/pdf/2412.12079},
        selected={true},
        preview={uniloc.gif}
      }